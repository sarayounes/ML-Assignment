{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35roXDEMudbw"
   },
   "source": [
    "# GUC Clustering Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIiItKbYudb2"
   },
   "source": [
    "**Objective:** \n",
    "The objective of this project teach students how to apply clustering to real data sets\n",
    "\n",
    "The projects aims to teach student: \n",
    "* Which clustering approach to use\n",
    "* Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures  \n",
    "* How to tune the parameters of each data approach\n",
    "* What is the effect of different distance functions (optional) \n",
    "* How to evaluate clustering approachs \n",
    "* How to display the output\n",
    "* What is the effect of normalizing the data \n",
    "\n",
    "Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtHElDYdudb3"
   },
   "outputs": [],
   "source": [
    "# if plotnine is not installed in Jupter then use the following command to install it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RHS5ZoQudb4"
   },
   "source": [
    "Running this project require the following imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T17:36:29.399956200Z",
     "start_time": "2024-03-04T17:36:29.391866800Z"
    },
    "id": "QrueqJenudb5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "from sklearn.datasets import make_blobs\n",
    "from plotnine import *   \n",
    "# StandardScaler is a function to normalize the data \n",
    "# You may also check MinMaxScaler and MaxAbsScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju2Zj6-nudb5"
   },
   "outputs": [],
   "source": [
    "# helper function that allows us to display data in 2 dimensions an highlights the clusters\n",
    "def display_cluster(X,km=[],num_clusters=0):\n",
    "    color = ['b', 'r', 'g', 'c', 'm', 'y', 'k', 'orange', 'pink', 'purple']  # List of colors\n",
    "    alpha = 0.5  #color obaque\n",
    "\n",
    "    s = 20\n",
    "    if num_clusters == 0:\n",
    "        plt.scatter(X[:,0],X[:,1],c = color[0],alpha = alpha,s = s, label='Data')\n",
    "    else:\n",
    "        X_np = X.values if isinstance(X, pd.DataFrame) else X  \n",
    "        for i in range(num_clusters):\n",
    "            cluster_indices = np.where(km.labels_ == i)[0]\n",
    "            plt.scatter(X_np[cluster_indices, 0], X_np[cluster_indices, 1], c=color[i], alpha=alpha, s=s)\n",
    "            plt.scatter(km.cluster_centers_[i][0], km.cluster_centers_[i][1], c=color[i], marker='x', s=100)\n",
    "    # plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDSIGjubudb8"
   },
   "source": [
    "## Multi Blob Data Set \n",
    "* The Data Set generated below has 6 cluster with varying number of users and varing densities\n",
    "* Cluster the data set below using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "n_bins = 6  \n",
    "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
    "                  centers=centers, shuffle=False, random_state=42)\n",
    "display_cluster(Multi_blob_Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDSIGjubudb8"
   },
   "source": [
    "### Kmeans \n",
    "* Use Kmeans with different values of K to cluster the above data \n",
    "* Display the outcome of each value of K \n",
    "* Plot distortion function versus K and choose the approriate value of k \n",
    "* Plot the silhouette_score versus K and use it to choose the best K \n",
    "* Store the silhouette_score for the best K for later comparison with other clustering techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "outputs": [],
   "source": [
    "# Multi-blob dataset K-means Clustering\n",
    "\n",
    "n_bins = 6\n",
    "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100, 150, 300, 400, 300, 200], n_features=2, cluster_std=[1.3, 0.6, 1.2, 1.7, 0.9, 1.7],\n",
    "                                 centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "distortions = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        km.fit(Multi_blob_Data)\n",
    "        distortions.append(km.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(Multi_blob_Data, km.labels_))\n",
    "\n",
    "        # to display clusters for each K\n",
    "        plt.figure()\n",
    "        plt.title(f'KMeans Clustering with K={k}')\n",
    "        display_cluster(Multi_blob_Data, km, k)\n",
    "    \n",
    "# to plot the distortion function versus K\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 11), distortions, marker='o')\n",
    "plt.title('Distortion vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# to plot the silhouette score versus K\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose the best K based on silhouette score\n",
    "best_K = range(2, 11)[np.argmax(silhouette_scores)]\n",
    "print(f\"The best K is {best_K} with a silhouette score of {np.max(silhouette_scores)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE7dvpOAudb9"
   },
   "source": [
    "### Hierarchal Clustering\n",
    "* Use AgglomerativeClustering function to  to cluster the above data \n",
    "* In the  AgglomerativeClustering change the following parameters \n",
    "    * Affinity (use euclidean, manhattan and cosine)\n",
    "    * Linkage( use average and single )\n",
    "    * Distance_threshold (try different)\n",
    "* For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters  \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-blob dataset Hierarchical Clustering\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "n_bins = 6\n",
    "centers = [(-3, -3), (0, 0), (5, 2.5), (-1, 4), (4, 6), (9, 7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100, 150, 300, 400, 300, 200], n_features=2,\n",
    "                                 cluster_std=[1.3, 0.6, 1.2, 1.7, 0.9, 1.7], centers=centers,\n",
    "                                 shuffle=False, random_state=42)\n",
    "\n",
    "data = pd.DataFrame(Multi_blob_Data)\n",
    "\n",
    "def hierarchical_clustering(data):\n",
    "    # Parameters\n",
    "    affinities = ['euclidean', 'cityblock', 'cosine']\n",
    "    linkages_methods = ['average', 'single']\n",
    "    distance_thresholds = [None]\n",
    "    diff_distance_thresholds = [10, 20]\n",
    "\n",
    "    best_score = -1\n",
    "    best_params = {}\n",
    "\n",
    "    for affinity in affinities:\n",
    "        for linkage_method in linkages_methods:\n",
    "            for distance_threshold in diff_distance_thresholds:\n",
    "                # to perform clustering\n",
    "                model = AgglomerativeClustering(affinity=affinity, linkage=linkage_method, distance_threshold=distance_threshold, n_clusters=None)\n",
    "                Z = linkage(data, method=linkage_method, metric=affinity)\n",
    "                clusters = model.fit_predict(data)\n",
    "    \n",
    "                # Calculate silhouette score\n",
    "                if len(np.unique(clusters)) > 1:\n",
    "                    score = silhouette_score(data, clusters)\n",
    "                    print(f\"Silhouette Score: {score}\")\n",
    "    \n",
    "                    # Plot dendrograms\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.title(f\"Dendrogram: Affinity={affinity}, Linkage={linkage_method}, Distance Threshold={distance_threshold}\")\n",
    "                    dendrogram(Z, truncate_mode='level', p=3)\n",
    "                    plt.xlabel(\"Sample Index\")\n",
    "                    plt.ylabel(\"Distance\")\n",
    "                    plt.show()\n",
    "    \n",
    "                    # Plot cluster results\n",
    "                    for pair in combinations(range(data.shape[1]), 2):\n",
    "                        plt.figure(figsize=(8, 6))\n",
    "                        plt.scatter(data.iloc[:, pair[0]], data.iloc[:, pair[1]], c=clusters, cmap='viridis')\n",
    "                        plt.title(f\"Clusters - Features {pair[0]} and {pair[1]} - Affinity: {affinity}, Linkage: {linkage_method}, Distance Threshold: {distance_threshold}\")\n",
    "                        plt.xlabel(f'Feature {pair[0]}')                       \n",
    "                        plt.ylabel(f'Feature {pair[1]}')\n",
    "                        plt.colorbar(label='Cluster')\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "\n",
    "    \n",
    "                    # Update best score and parameters if applicable\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\n",
    "                            'Affinity': affinity,\n",
    "                            'Linkage': linkage_method,\n",
    "                            'Distance Threshold': distance_threshold\n",
    "                        }\n",
    "    \n",
    "       # Iterate over parameter combinations with distance_thresholds\n",
    "    for affinity in affinities:\n",
    "        for linkage_method in linkages_methods:\n",
    "            for distance_threshold in distance_thresholds:\n",
    "\n",
    "                if distance_threshold==None:\n",
    "                   n_clusters = 2\n",
    "                else:\n",
    "                   n_clusters = None\n",
    "                \n",
    "                # Agglomerative Clustering\n",
    "                model = AgglomerativeClustering(affinity=affinity, linkage=linkage_method,\n",
    "                                                     distance_threshold=distance_threshold, n_clusters=n_clusters)\n",
    "                clusters = model.fit_predict(data)\n",
    "                Z = linkage(data, method=linkage_method, metric=affinity)\n",
    "\n",
    "                # to check if multiple clusters are formed\n",
    "                if len(np.unique(clusters)) > 1:\n",
    "                    # Calculate silhouette score\n",
    "                    score = silhouette_score(data, clusters)\n",
    "                    print(f\"Silhouette score: {score}\")\n",
    "\n",
    "                    # Plot dendrogram\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    plt.title(f'Dendrogram - Affinity: {affinity}, Linkage: {linkage_method}, Distance Threshold: {distance_threshold}')\n",
    "                    dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\n",
    "                    plt.xlabel('Sample Index')\n",
    "                    plt.ylabel('Distance')\n",
    "                    plt.show()\n",
    "\n",
    "                    # Plot resulting clusters \n",
    "                    for pair in combinations(range(data.shape[1]), 2):\n",
    "                        plt.figure(figsize=(8, 6))\n",
    "                        plt.scatter(data.iloc[:, pair[0]], data.iloc[:, pair[1]], c=clusters, cmap='viridis')\n",
    "                        plt.title(f\"Clusters - Features {pair[0]} and {pair[1]} - Affinity: {affinity}, Linkage: {linkage_method}, Distance Threshold: {distance_threshold}\")\n",
    "                        plt.xlabel(f'Feature {pair[0]}')\n",
    "                        plt.ylabel(f'Feature {pair[1]}')\n",
    "                        plt.colorbar(label='Cluster')\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\n",
    "                            'Affinity': affinity,\n",
    "                            'Linkage': linkage_method,\n",
    "                            'Distance Threshold': distance_threshold\n",
    "                        }\n",
    "                    \n",
    "        # Print the best silhouette score and its corresponding parameters\n",
    "        print(\"Best Silhouette Score:\", best_score)\n",
    "        print(\"Best Parameters:\", best_params)\n",
    "\n",
    "hierarchical_clustering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myJE7vQKudb-"
   },
   "source": [
    "### DBScan\n",
    "* Use DBScan function to  to cluster the above data \n",
    "* In the  DBscan change the following parameters \n",
    "    * EPS (from 0.1 to 3)\n",
    "    * Min_samples (from 5 to 25)\n",
    "* Plot the silhouette_score versus the variation in the EPS and the min_samples\n",
    "* Plot the resulting Clusters in this case \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observations and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Multi-blob dataset DBSCAN \n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "n_bins = 6\n",
    "centers = [(-3, -3), (0, 0), (5, 2.5), (-1, 4), (4, 6), (9, 7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100, 150, 300, 400, 300, 200], n_features=2,\n",
    "                                 cluster_std=[1.3, 0.6, 1.2, 1.7, 0.9, 1.7], centers=centers,\n",
    "                                 shuffle=False, random_state=42)\n",
    "\n",
    "data= pd.DataFrame(Multi_blob_Data)\n",
    "\n",
    "def dbscan_clustering(data):\n",
    "    # Parameters\n",
    "    eps_values = np.linspace(0.1, 3, 10)\n",
    "    min_samples_values = range(5, 26)\n",
    "\n",
    "    best_score = -1  \n",
    "    best_params = { }\n",
    "    best_clusters = None\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            # Perform DBSCAN clustering\n",
    "            model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            clusters = model.fit_predict(data)\n",
    "\n",
    "            if len(np.unique(clusters)) <= 1:\n",
    "                continue\n",
    "\n",
    "            # Calculate silhouette score\n",
    "            score = silhouette_score(data, clusters)\n",
    "\n",
    "            # Store silhouette score and parameters\n",
    "            silhouette_scores.append(score)\n",
    "\n",
    "            if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\n",
    "                            'EPS': eps,\n",
    "                            'Min Samples': min_samples,\n",
    "                        }\n",
    "                        best_clusters= clusters\n",
    "                \n",
    "            # Plot the resulting clusters for each pair of features\n",
    "            n_features = data.shape[1]\n",
    "            for i in range(n_features):\n",
    "                for j in range(i+1,n_features):\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    plt.scatter(data.iloc[:, i], data.iloc[:, j], c=clusters, cmap='viridis', s=50, alpha=0.5)\n",
    "                    plt.xlabel(data.columns[i])\n",
    "                    plt.ylabel(data.columns[j])\n",
    "                    plt.colorbar(label='Cluster')\n",
    "                    plt.grid(True)\n",
    "                    plt.tight_layout()\n",
    "                    plt.axis\n",
    "                    plt.show()\n",
    "\n",
    "    # Plot silhouette score versus eps and min_samples\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(silhouette_scores)), silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title('Silhouette Score vs EPS and Min Samples')\n",
    "    plt.xlabel('Parameter Combination')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(f\"Best Silhouette Score: {best_score} with {best_params}\")\n",
    "\n",
    "\n",
    "dbscan_clustering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip16g1QFudb_"
   },
   "source": [
    "### Gaussian Mixture\n",
    "* Use GaussianMixture function to cluster the above data \n",
    "* In GMM change the covariance_type and check the difference in the resulting proabability fit \n",
    "* Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-blob dataset GMM Clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def gaussian_mixture_clustering(data):\n",
    "    n_components = 3\n",
    "    covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "    best_score = -1\n",
    "    best_params = {}\n",
    "    silhouette_scores = []\n",
    "    parameters = []\n",
    "\n",
    "    for covariance_type in covariance_types:\n",
    "        # Perform clustering\n",
    "        gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type)\n",
    "        cluster_labels = gmm.fit_predict(data)\n",
    "        # best_cluster_labels = cluster_labels\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        score = silhouette_score(data, cluster_labels)\n",
    "        silhouette_scores.append(score)\n",
    "        parameters.append({'Covariance Type': covariance_type})\n",
    "\n",
    "        # Update best score and parameters if applicable\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'Covariance Type': covariance_type}\n",
    "            best_cluster_labels= cluster_labels\n",
    "\n",
    "    # Plot scatter plot for each pair of features\n",
    "    n_features = data.shape[1]\n",
    "    for i in range(n_features):\n",
    "        for j in range(i + 1, n_features):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            # Scatter plot\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(data.iloc[:, i], data.iloc[:, j], c=best_cluster_labels, cmap='viridis', s=50, alpha=0.5)\n",
    "            plt.xlabel(f'Feature {i}')\n",
    "            plt.ylabel(f'Feature {j}')\n",
    "            plt.title('Scatter Plot')\n",
    "\n",
    "            # Fit GaussianMixture model for the current pair of features\n",
    "            gmm_pair = GaussianMixture(n_components=3, covariance_type=best_params['Covariance Type'])\n",
    "            X_pair = data[[data.columns[i], data.columns[j]]]\n",
    "            gmm_pair.fit(X_pair)\n",
    "\n",
    "            # Contour plot for Gaussian mixture\n",
    "            plt.subplot(1, 2, 2)\n",
    "            x_min, x_max = data.iloc[:, i].min() - 1, data.iloc[:, i].max() + 1\n",
    "            y_min, y_max = data.iloc[:, j].min() - 1, data.iloc[:, j].max() + 1\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                                 np.linspace(y_min, y_max, 100))\n",
    "            Z = gmm_pair.score_samples(np.column_stack([xx.ravel(), yy.ravel()]))\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            plt.contourf(xx, yy, Z, cmap='viridis', levels=20, alpha=0.5)\n",
    "            plt.xlabel(f'Feature {i}')\n",
    "            plt.ylabel(f'Feature {j}')\n",
    "            plt.title('Contour Plot')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # Plot silhouette score versus covariance type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(covariance_types, silhouette_scores, color='skyblue')\n",
    "    plt.xlabel('Covariance Type')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score versus Covariance Type')\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(f\"Best Silhouette Score: {best_score} with {best_params}\")\n",
    "\n",
    "gaussian_mixture_clustering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m92lZkkyudb_"
   },
   "source": [
    "## iris data set \n",
    "The iris data set is test data set that is part of the Sklearn module \n",
    "which contains 150 records each with 4 features. All the features are represented by real numbers \n",
    "\n",
    "The data represents three classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QaCWyyCudcA",
    "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "iris_data.target[[10, 25, 50]]\n",
    "#array([0, 0, 1])\n",
    "list(iris_data.target_names)\n",
    "['setosa', 'versicolor', 'virginica']\n",
    "X_df = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyoCVfyMudcA"
   },
   "source": [
    "* Repeat all the above clustering approaches and steps on the above data \n",
    "* Normalize the data then repeat all the above steps \n",
    "* Compare between the different clustering approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris data K-means clusters\n",
    "\n",
    "distortions = [] \n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2,11):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X_df)\n",
    "    distortions.append(km.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_df, km.labels_))\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(X_df, km, k)\n",
    "\n",
    "# Plot distortion function versus no of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2,11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus Number of Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Plot silhouette score versus no of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2,11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = range(2,11)[np.argmax(silhouette_scores)]\n",
    "print(f\"The best K is {best_K} with a silhouette score of {np.max(silhouette_scores)}\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris data Hierarchical Clustering\n",
    "hierarchical_clustering(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris data DBSCAN Clustering\n",
    "dbscan_clustering(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris data GMM Clustering\n",
    "gaussian_mixture_clustering(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize iris data \n",
    "scaler=MinMaxScaler()\n",
    "X_Normalized=scaler.fit_transform(X_df)\n",
    "X_df_Normalized=pd.DataFrame(X_Normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized iris data K-means clustering\n",
    "\n",
    "distortions = [] \n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2,11):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X_df_Normalized)\n",
    "    distortions.append(km.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_df_Normalized, km.labels_))\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(X_df_Normalized, km, k)\n",
    "\n",
    "# Plot distortion function versus no of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2,11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus Number of Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot silhouette score versus no of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2,11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = range(2,11)[np.argmax(silhouette_scores)]\n",
    "print(f\"The best K is {best_K} with a silhouette score of {np.max(silhouette_scores)}\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized iris data Hierarchical clustering\n",
    "hierarchical_clustering(X_df_Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized iris data DBSCAN Clustering \n",
    "dbscan_clustering(X_df_Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized iris data GMM Clustering\n",
    "gaussian_mixture_clustering(X_df_Normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare between the different clustering approaches\n",
    "\n",
    "Before Normalization:\n",
    "* Hierarchical Clustering and DBSCAN achieved the highest silhouette score of 0.687.\n",
    "* K Means Clustering also performed well with a silhouette score of 0.681.\n",
    "* GMM had a lower silhouette score of 0.553 compared to other methods.\n",
    "\n",
    "After Normalization:\n",
    "* All methods achieved similar silhouette scores around 0.630.\n",
    "* Hierarchical Clustering, DBSCAN, and K Means Clustering had identical silhouette scores and parameters.\n",
    "* GMM had a slightly lower silhouette score of 0.507 with a different covariance type.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2oBmWT2udcA"
   },
   "source": [
    "## Customer dataset\n",
    "Repeat all the above on the customer data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T17:02:54.398110900Z",
     "start_time": "2024-03-04T17:02:54.387932500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "customer_data = pd.read_csv(\"/Users/HP/Desktop/Customer data.csv\")\n",
    "customer_data.set_index('ID', inplace=True)\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer data K-means Clustering\n",
    "\n",
    "distortions = [] \n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2,11):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(customer_data)\n",
    "    distortions.append(km.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(customer_data, km.labels_))\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(customer_data, km, k)\n",
    "\n",
    "# Plot distortion function versus no of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2,11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus Number of Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot silhouette score versus no of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2,11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = range(2,11)[np.argmax(silhouette_scores)]\n",
    "print(f\"The best K is {best_K} with a silhouette score of {np.max(silhouette_scores)}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer data DBSCAN Clustering\n",
    "dbscan_clustering(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer data GMM Clustering\n",
    "gaussian_mixture_clustering(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Customer Data\n",
    "customer_data_Normalized=scaler.fit_transform(customer_data)\n",
    "customer_data_Normalized_df = pd.DataFrame(customer_data_Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Customer data K-means clustering\n",
    "\n",
    "distortions = [] \n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2,11):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(customer_data_Normalized_df)\n",
    "    distortions.append(km.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(customer_data_Normalized_df, km.labels_))\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(customer_data_Normalized_df, km, k)\n",
    "\n",
    "# Plot distortion function versus no of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2,11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus Number of Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot silhouette score versus no of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2,11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.grid(True)\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = range(2,11)[np.argmax(silhouette_scores)]\n",
    "print(f\"The best K is {best_K} with a silhouette score of {np.max(silhouette_scores)}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Customer data Hierarchical clustering\n",
    "hierarchical_clustering(customer_data_Normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Customer data DBSCAN clustering\n",
    "dbscan_clustering(customer_data_Normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Customer data GMM clustering\n",
    "gaussian_mixture_clustering(customer_data_Normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison\n",
    "\n",
    "Before Normalization:\n",
    "* Hierarchical Clustering achieved the highest silhouette score of 0.759, followed by K Means Clustering with 0.583.\n",
    "* DBSCAN failed to find clusters, resulting in a silhouette score of -1.\n",
    "* GMM had a silhouette score of 0.483.\n",
    "\n",
    "After Normalization: \n",
    "* DBSCAN performed the best with a silhouette score of 0.521, followed by K Means Clustering with 0.437.\n",
    "* Hierarchical Clustering and GMM had lower silhouette scores compared to DBSCAN and K Means Clustering.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
